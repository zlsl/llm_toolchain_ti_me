# Набор скриптов для файнтюна LLM моделей для ролеплеев по технологии TiMe. 


## Подготовка модели

В токенизер и модель добавляется новый токен `<char>`. Данный токен добавляется в начало каждой строки диалога датасета вместо "- "

Было:

>\- Привет, как дела?

Стало:

>`<char>`Привет, как дела?

Используется скрипт `model_add_tokens.sh`. В коде необходимо указать имена исходной и новой модели
```
SRC_MODEL="gpt2_small" # Каталог с исходной моделью
NEW_MODEL="gpt2_small_for_train" # Имя новой модели
```

В результате будет создана новая модель с добавленым токеном `<char>`

## Описание технологии

Основной проблемой для создание разговорных моделей является обилие имён собственных в тексте. Иногда модель может создать новый текст, поняв, что в исходном датасете некий `Вася` - должен стать личностью модели. Но имён много и модель не имеет возможности понять, что от неё хотят.

Для решения этой проблемы была придумана технология TiMe (анаграмма Ты - Я).

Суть состоит в том, что из исходного текстового датасета выделяются все имена собственные, далее они преобразовываются в два дополнительных датасета, где в одном датасете Имена собственные приведены к местоимениям (Я, мне, меня, мной и т.д.), а во втором к Ты, тебе, тобой, и т.д.

В итоге, фраза вида:

> Вася открыл дверь и задумался

Преобразуется в две:
 
> Я открыл дверь и задумался

> Ты открыл дверь и задумался

Далее к исходному датасету добавляются оба новых и этот объединённый датасет используется для обучения модели. Большое количчество параметров модели (даже для gpt2_small) позволяют практически полностью сгладить погрешности при обработке части фраз диалогов. Таких как:

>Меня зовут Вася

Не корректно преобразуется к:

>Меня зовут я


Также прямая речь преобразуется к новому формату:

>\- Привет, - сказал Вася и улыбнулся, - что у нас сегодня на ужин?

В

>`<code>`Привет (сказал Вася и улыбнулся) что у нас сегодня на ужин?

Таким образом при генерации на выходе будет не кусок текста из рассказа, а вполне удобная и "человеческая" реплика. А в комплексе с TiMe получится:

>`<code>`Привет (сказал я и улыбнулся) что у нас сегодня на ужин?

## Скрипт обработки датасета

Основной скрипт `dataset_tools/make_datasets.sh`

В коде при необходимости откорректируйте имена файлов:
```
SRC_FILE="raw.txt"
BASE_NAME="dataset"
```

После запуска скрипта их исходного текстового файла будут созданы:

- `dataset.txt` - новый датасет для файнтюна
- `dialogs.txt` - базовый датасет с добавленым токеном `<char>` и переформатированной прямой речью
- `raw_dialogs.txt` - базовый датасет с добавленым символом `@` в начале диалога и переформатированной прямой речью (вместо скобок символ `|`)
- `me_dialogs.txt` `ti_dialogs.txt` - базовый датасет с добавленым токеном `<char>` и переформатированной прямой речью с заменой имён собственных на местоимения
- `dict_me.py` `dict_ti.py` - файлы словарей с сопоставлением имени собственного к местоимению. **Важно!** Для нестандартных имён потребуется корректировка этих файлов и запуск процесса с этапа `Создание датасетов TiMe`

Есть возможность создать датасет в стиле альпаки. Для этого запустить скрипт:

`make_alpaca.py raw_dialogs.txt dataset.json`

## Файнтюн модели

Для удобства создан скрипт `model_train_gpt2.sh`

В коде настраиваются параметры:

```
DATASET_FILE="dataset.txt" - имя файла датасета
SRC_MODEL="gpt2_small_for_train" - каталог модели для файнтюна
PROJECT_NAME="finetune" - имя проекта (каталога новой модели)
SAVE_STEPS=500
BATCH_SIZE=1
GAS=1 - gradient accumulation steps
WEIGHT_DECAY="0.03"
LR="0.00006" - Learning rate
OPTIMIZER="adamw_bnb_8bit"
```

Для GPU 1060-1080ti скорость обучения выше при использовании fp32, для более новых моделей лучше активировать параметр `--fp16`

